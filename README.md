# YOLO Training & Inference Service

A modular, three-layer ML service for training, validating, and deploying YOLO models. Supports **YOLOv5, v8, v9, v10, v11, and YOLO-World** via the Ultralytics library. Includes a composable spatial inference pipeline for real-time detection on wearables.

## ğŸŒ Hybrid AI: Cloud + Local Edge

This system is designed to bridge the gap between high-power cloud computing and low-latency edge devices (glasses, phones, IoT).

- **Cloud/Server Role (The Brain)**: 
  - **Heavy Training**: Run multi-GPU training jobs for large backbone models.
  - **High-Accuracy Inference**: Serve complex models via the FastAPI `/api/v1/infer` endpoint for remote devices.
  - **PEFT Training**: Fine-tune "Personal Object" models with LoRA/BitFit using minimal data from the edge.
- **Local Edge Role (The Reflex)**:
  - **Lightweight SLAM**: Track 3D world coordinates and camera pose locally without internet latency.
  - **Quantized Export**: Use the `ExportService` to shrink models into **NCNN, TFLite, or CoreML** for 8-bit on-device execution.
  - **Spatial Anchoring**: "Lock" cloud detections to the 3D world so they remain stable even when the user looks away.

## ğŸ— Architecture

The project follows a strict three-layer architecture to ensure testability and separation of concerns:

- **Service Layer (`service/`)**: Pure business logic. Handles YOLO training (via Ultralytics), inference operations, SLAM spatial tracking, and dataset structure detection.
- **API Layer (`api/`)**: HTTP endpoints, request/response validation using Pydantic schemas, and serialization.
- **Server Layer (`server/`)**: Application bootstrap, FastAPI configuration, CORS middleware, and Uvicorn entrypoint.
- **Pipeline Layer (`pipeline/`)**: Composable, chainable pipelines for real-time inference with spatial awareness.

For more details, see [ARCHITECTURE.md](ARCHITECTURE.md).

## ğŸš€ Key Features

- **Multi-YOLO Support**: Works with YOLOv5, v8, v9, v10, v11, and YOLO-World models.
- **3D Spatial Intelligence (SLAM)**: Real-time 6DoF camera pose estimation and 3D object anchoring for wearables.
- **PEFT / Personal Learning**: Train custom "Private" models with 5-10 images using **LoRA** (Low-Rank Adaptation).
- **Automated Dataset Prep**: Integration with Kaggle for dataset downloads and automatic YOLO structure detection.
- **Preprocessing Pipeline**: Composable cleaners (corrupted image detection, bbox validation) and transforms (augmentation).
- **Deployment-Ready Exports**: Export trained models to NCNN, ONNX, CoreML, and TFLite formats for edge hardware.
- **Interactive Counting**: Human-in-the-loop density refinement based on [arxiv:2309.05277](https://arxiv.org/abs/2309.05277).
- **Advanced AI Capabilities**: Gaze-prioritized learning and dense counting with SimOTA. See [CAPABILITIES.md](docs/CAPABILITIES.md).

## ğŸ›  Setup

### Prerequisites
- Python 3.9+
- [uv](https://docs.astral.sh/uv/) (recommended) or pip
- CUDA-compatible environment (optional, but recommended for training)

### Installation with uv (Recommended)
```bash
cd yolo-training

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

### Installation with pip (Alternative)
```bash
cd yolo-training
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## ğŸš¦ Running the Service

Start the FastAPI server:
```bash
python server/main.py
```

The API will be available at `http://localhost:8000`.
- **Interactive Docs (Swagger)**: `http://localhost:8000/docs`
- **Alternative Docs (Redoc)**: `http://localhost:8000/redoc`

## ğŸ”— Composable Pipelines

The `pipeline/` module provides chainable, attachable pipelines for real-time inference:

```python
from pipeline import SpatialInferencePipeline, AttachablePipeline, FunctionStage

# High-level: YOLO + SLAM spatial detection
pipeline = SpatialInferencePipeline(
    model_path="resources/yolov8m.pt",  # or yolov11, yolov5, etc.
    enable_slam=True,
    imu_enabled=True,
)

for frame in video_stream:
    result = pipeline.process_frame(frame, imu_data=sensor_data)
    for det in result.detections:
        print(f"{det.class_name} at {det.spatial_coords}")

# Low-level: Compose custom pipelines
pipeline = AttachablePipeline()
pipeline.attach("normalize", FunctionStage("norm", lambda x, ctx: x / 255.0))
pipeline.attach("inference", InferenceStage("model.pt"))
# Dynamically attach/detach stages at runtime
```

## ğŸ§ª Testing

The project includes both standalone and `pytest`-based tests:

### All Tests
```bash
# Run all tests
python service/slam/tests/test_slam_service.py
python pipeline/tests/test_base.py
python pipeline/tests/test_spatial_inference.py
python api/tests/run_tests.py
```

### With pytest
```bash
pytest api/tests/ -v
pytest service/preprocessing/tests/ -v
```

## ğŸ“‚ Project Structure

```text
yolo-training/
â”œâ”€â”€ api/                # HTTP Layer (FastAPI)
â”‚   â”œâ”€â”€ routes.py       # API Endpoints
â”‚   â”œâ”€â”€ schemas.py      # Pydantic Models
â”‚   â””â”€â”€ tests/          # API Test Suite
â”œâ”€â”€ server/             # Bootstrap Layer
â”‚   â””â”€â”€ main.py         # Entry point
â”œâ”€â”€ service/            # Core Logic Layer
â”‚   â”œâ”€â”€ preprocessing/  # Data Cleaning & Augmentation
â”‚   â”œâ”€â”€ slam/           # Spatial tracking (SLAM)
â”‚   â”œâ”€â”€ training_service.py
â”‚   â”œâ”€â”€ inference_service.py
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ pipeline/           # Composable Pipelines
â”‚   â”œâ”€â”€ base.py         # Pipeline abstractions
â”‚   â”œâ”€â”€ spatial_inference.py  # YOLO + SLAM pipeline
â”‚   â””â”€â”€ tests/
â””â”€â”€ requirements.txt
```
